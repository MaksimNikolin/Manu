{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4891ad1a-6bc1-4e6b-b3b6-d0f52d0acb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2140abac-64bb-43ff-a00b-dfb6864c0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def extract_label(label):\n",
    "    return [b['labels'] for b in label]\n",
    "\n",
    "def transform_pic_name(pic_name):\n",
    "    match = re.search(r'(\\d+)(?=.jpg)', pic_name)\n",
    "    if match:\n",
    "        return match.group(1) + '.jpg'\n",
    "    return pic_name\n",
    "\n",
    "def image_to_tensor(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('L')\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomRotation(15),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor\n",
    "\n",
    "def apply_image_to_tensor(row):\n",
    "    img_path = os.path.join(image_dir, row['pic_name'])\n",
    "    return image_to_tensor(img_path)\n",
    "\n",
    "def tensor_to_columns(tensor):\n",
    "    tensor_list = tensor.view(-1).tolist()\n",
    "    column_names = [f'pixel_{i+1}' for i in range(len(tensor_list))]\n",
    "    return pd.Series(tensor_list, index=column_names)\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3e0154-73cb-4377-90fe-ab8e35ca44ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>pixel_1</th>\n",
       "      <th>pixel_2</th>\n",
       "      <th>pixel_3</th>\n",
       "      <th>pixel_4</th>\n",
       "      <th>pixel_5</th>\n",
       "      <th>pixel_6</th>\n",
       "      <th>pixel_7</th>\n",
       "      <th>pixel_8</th>\n",
       "      <th>pixel_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_775</th>\n",
       "      <th>pixel_776</th>\n",
       "      <th>pixel_777</th>\n",
       "      <th>pixel_778</th>\n",
       "      <th>pixel_779</th>\n",
       "      <th>pixel_780</th>\n",
       "      <th>pixel_781</th>\n",
       "      <th>pixel_782</th>\n",
       "      <th>pixel_783</th>\n",
       "      <th>pixel_784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   pixel_1   pixel_2   pixel_3   pixel_4   pixel_5   pixel_6  \\\n",
       "0      73  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "1      72  0.992157  0.992157  0.992157  0.992157  1.000000  0.992157   \n",
       "2      71  0.984314  0.984314  0.984314  1.000000  0.984314  1.000000   \n",
       "3      65  0.992157  0.992157  0.992157  0.992157  0.992157  1.000000   \n",
       "4      66  0.992157  0.992157  0.992157  1.000000  0.984314  0.984314   \n",
       "\n",
       "    pixel_7   pixel_8   pixel_9  ...  pixel_775  pixel_776  pixel_777  \\\n",
       "0  1.000000  1.000000  1.000000  ...        1.0        1.0        1.0   \n",
       "1  0.992157  0.992157  0.992157  ...        1.0        1.0        1.0   \n",
       "2  1.000000  1.000000  1.000000  ...        1.0        1.0        1.0   \n",
       "3  1.000000  0.992157  0.992157  ...        1.0        1.0        1.0   \n",
       "4  0.992157  1.000000  0.984314  ...        1.0        1.0        1.0   \n",
       "\n",
       "   pixel_778  pixel_779  pixel_780  pixel_781  pixel_782  pixel_783  pixel_784  \n",
       "0        1.0        1.0        1.0        1.0        1.0        1.0        1.0  \n",
       "1        1.0        1.0        1.0        1.0        1.0        1.0        1.0  \n",
       "2        1.0        1.0        1.0        1.0        1.0        1.0        1.0  \n",
       "3        1.0        1.0        1.0        1.0        1.0        1.0        1.0  \n",
       "4        1.0        1.0        1.0        1.0        1.0        1.0        1.0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing\n",
    "file_path = 'marked_df.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "#from marked_df to train_df\n",
    "df = df.drop(['annotation_id', 'annotator', 'created_at', 'id', 'updated_at', 'lead_time', 'bbox'], axis=1)\n",
    "df = df.rename(columns={'transcription': 'target', 'ocr': 'pic_name'})\n",
    "df['label'] = df['label'].apply(ast.literal_eval)\n",
    "df['label'] = df['label'].apply(extract_label)\n",
    "\n",
    "train_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    for label in row['label']:\n",
    "        train_data.append({\n",
    "            'pic_name': row['pic_name'],\n",
    "            'target': label\n",
    "        })\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "#pic_name\n",
    "train_df['new_pic_name'] = train_df['pic_name'].apply(transform_pic_name)\n",
    "train_df.drop('pic_name', axis=1, inplace=True)\n",
    "train_df.rename(columns={'new_pic_name' : 'pic_name'}, inplace=True)\n",
    "train_df.to_csv('df.csv', index=False)\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "\n",
    "#image2tensor\n",
    "image_dir = 'data/'\n",
    "df['image_tensor'] = df.apply(apply_image_to_tensor, axis=1)\n",
    "\n",
    "#target2numeric\n",
    "label_encoder = LabelEncoder()\n",
    "df['target'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "#tensor2columns\n",
    "df_tensor = df['image_tensor'].apply(tensor_to_columns)\n",
    "df = pd.concat([df, df_tensor], axis=1)\n",
    "df.drop(columns=['image_tensor', 'pic_name'], inplace=True)\n",
    "df.to_csv('df.csv', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e8d733-f785-43fa-a3a5-c32635f62ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218, 785)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb59fbb-e30b-4016-b2e3-8a6bd8b1eb24",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cd00f9-7a81-4105-9cee-2742568a45d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (174, 784)\n",
      "Shape X_val: (44, 784)\n",
      "Shape y_train: (174,)\n",
      "Shape y_val: (44,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(df)\n",
    "\n",
    "print(f'Shape X_train: {X_train.shape}')\n",
    "print(f'Shape X_val: {X_val.shape}')\n",
    "print(f'Shape y_train: {y_train.shape}')\n",
    "print(f'Shape y_val: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5eb9284-ae87-49fc-912e-67be25983bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).view(-1, 1, 28, 28)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).view(-1, 1, 28, 28)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b9d1af-6ecc-4eb6-8695-1352d9474d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=74, bias=True)\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class modelCNN(nn.Module):\n",
    "    def __init__(self, num_classes=74):\n",
    "        super(modelCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, num_classes)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model1 = modelCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e3d855-f087-4bd6-aa9b-f72a7f21e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 6.8999, Train Accuracy: 0.00%\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [2/100], Loss: 5.3190, Train Accuracy: 1.15%\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [3/100], Loss: 4.7478, Train Accuracy: 1.15%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [4/100], Loss: 4.4655, Train Accuracy: 3.45%\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [5/100], Loss: 4.2036, Train Accuracy: 6.32%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [6/100], Loss: 4.1996, Train Accuracy: 7.47%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [7/100], Loss: 4.0667, Train Accuracy: 4.02%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [8/100], Loss: 3.6725, Train Accuracy: 10.34%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [9/100], Loss: 3.7613, Train Accuracy: 9.20%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [10/100], Loss: 3.5531, Train Accuracy: 13.79%\n",
      "Validation Accuracy: 2.27%\n",
      "Epoch [11/100], Loss: 3.4127, Train Accuracy: 14.37%\n",
      "Validation Accuracy: 4.55%\n",
      "Epoch [12/100], Loss: 3.2637, Train Accuracy: 13.79%\n",
      "Validation Accuracy: 4.55%\n",
      "Epoch [13/100], Loss: 3.0986, Train Accuracy: 18.39%\n",
      "Validation Accuracy: 6.82%\n",
      "Epoch [14/100], Loss: 2.9381, Train Accuracy: 19.54%\n",
      "Validation Accuracy: 6.82%\n",
      "Epoch [15/100], Loss: 2.7171, Train Accuracy: 29.31%\n",
      "Validation Accuracy: 11.36%\n",
      "Epoch [16/100], Loss: 2.5344, Train Accuracy: 31.61%\n",
      "Validation Accuracy: 13.64%\n",
      "Epoch [17/100], Loss: 2.4829, Train Accuracy: 30.46%\n",
      "Validation Accuracy: 9.09%\n",
      "Epoch [18/100], Loss: 2.3010, Train Accuracy: 35.63%\n",
      "Validation Accuracy: 15.91%\n",
      "Epoch [19/100], Loss: 2.2877, Train Accuracy: 35.63%\n",
      "Validation Accuracy: 15.91%\n",
      "Epoch [20/100], Loss: 1.9943, Train Accuracy: 44.83%\n",
      "Validation Accuracy: 18.18%\n",
      "Epoch [21/100], Loss: 1.6612, Train Accuracy: 46.55%\n",
      "Validation Accuracy: 20.45%\n",
      "Epoch [22/100], Loss: 1.5461, Train Accuracy: 53.45%\n",
      "Validation Accuracy: 25.00%\n",
      "Epoch [23/100], Loss: 1.5465, Train Accuracy: 55.17%\n",
      "Validation Accuracy: 31.82%\n",
      "Epoch [24/100], Loss: 1.2273, Train Accuracy: 61.49%\n",
      "Validation Accuracy: 29.55%\n",
      "Epoch [25/100], Loss: 1.2299, Train Accuracy: 62.64%\n",
      "Validation Accuracy: 36.36%\n",
      "Epoch [26/100], Loss: 1.0849, Train Accuracy: 63.79%\n",
      "Validation Accuracy: 36.36%\n",
      "Epoch [27/100], Loss: 0.9964, Train Accuracy: 66.09%\n",
      "Validation Accuracy: 27.27%\n",
      "Epoch [28/100], Loss: 0.9790, Train Accuracy: 67.82%\n",
      "Validation Accuracy: 36.36%\n",
      "Epoch [29/100], Loss: 0.7988, Train Accuracy: 73.56%\n",
      "Validation Accuracy: 31.82%\n",
      "Epoch [30/100], Loss: 0.7723, Train Accuracy: 77.59%\n",
      "Validation Accuracy: 38.64%\n",
      "Epoch [31/100], Loss: 0.6838, Train Accuracy: 75.86%\n",
      "Validation Accuracy: 45.45%\n",
      "Epoch [32/100], Loss: 0.6373, Train Accuracy: 80.46%\n",
      "Validation Accuracy: 40.91%\n",
      "Epoch [33/100], Loss: 0.5252, Train Accuracy: 82.76%\n",
      "Validation Accuracy: 43.18%\n",
      "Epoch [34/100], Loss: 0.5646, Train Accuracy: 80.46%\n",
      "Validation Accuracy: 43.18%\n",
      "Epoch [35/100], Loss: 0.5147, Train Accuracy: 83.91%\n",
      "Validation Accuracy: 43.18%\n",
      "Epoch [36/100], Loss: 0.4980, Train Accuracy: 85.63%\n",
      "Validation Accuracy: 43.18%\n",
      "Epoch [37/100], Loss: 0.4160, Train Accuracy: 85.06%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [38/100], Loss: 0.3652, Train Accuracy: 89.08%\n",
      "Validation Accuracy: 47.73%\n",
      "Epoch [39/100], Loss: 0.3663, Train Accuracy: 90.23%\n",
      "Validation Accuracy: 47.73%\n",
      "Epoch [40/100], Loss: 0.3784, Train Accuracy: 87.93%\n",
      "Validation Accuracy: 45.45%\n",
      "Epoch [41/100], Loss: 0.4570, Train Accuracy: 86.78%\n",
      "Validation Accuracy: 45.45%\n",
      "Epoch [42/100], Loss: 0.2456, Train Accuracy: 93.68%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [43/100], Loss: 0.3144, Train Accuracy: 91.38%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [44/100], Loss: 0.3046, Train Accuracy: 90.80%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [45/100], Loss: 0.2719, Train Accuracy: 92.53%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [46/100], Loss: 0.2465, Train Accuracy: 94.25%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [47/100], Loss: 0.3279, Train Accuracy: 89.08%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [48/100], Loss: 0.2674, Train Accuracy: 92.53%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [49/100], Loss: 0.2368, Train Accuracy: 94.83%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [50/100], Loss: 0.2720, Train Accuracy: 93.10%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [51/100], Loss: 0.2686, Train Accuracy: 89.66%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [52/100], Loss: 0.3302, Train Accuracy: 89.08%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [53/100], Loss: 0.2533, Train Accuracy: 94.25%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [54/100], Loss: 0.2419, Train Accuracy: 94.25%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [55/100], Loss: 0.2452, Train Accuracy: 91.38%\n",
      "Validation Accuracy: 56.82%\n",
      "Epoch [56/100], Loss: 0.2276, Train Accuracy: 92.53%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [57/100], Loss: 0.2224, Train Accuracy: 94.83%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [58/100], Loss: 0.1807, Train Accuracy: 95.40%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [59/100], Loss: 0.2825, Train Accuracy: 91.95%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [60/100], Loss: 0.2422, Train Accuracy: 93.10%\n",
      "Validation Accuracy: 54.55%\n",
      "Epoch [61/100], Loss: 0.1820, Train Accuracy: 94.83%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [62/100], Loss: 0.2509, Train Accuracy: 91.95%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [63/100], Loss: 0.2130, Train Accuracy: 93.10%\n",
      "Validation Accuracy: 52.27%\n",
      "Epoch [64/100], Loss: 0.2121, Train Accuracy: 93.68%\n",
      "Validation Accuracy: 50.00%\n",
      "Epoch [65/100], Loss: 0.3232, Train Accuracy: 91.38%\n",
      "Validation Accuracy: 50.00%\n",
      "Early stopping triggered after 65 epochs with no improvement.\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, val_loader, epochs=100, device='cpu', patience=5):\n",
    "    \n",
    "    best_val_accuracy, epochs_without_improvement = 0.0, 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        \n",
    "        val_accuracy = val_model(model, val_loader, device)\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs with no improvement.\")\n",
    "            break\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "def val_model(model, val_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_accuracy:.2f}%')\n",
    "    return val_accuracy\n",
    "\n",
    "train_model(model1, train_loader, criterion, optimizer, val_loader, epochs=100, device='cpu', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fd9937-101b-4344-80e8-3ac2e1cdcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_prediction(model, val_loader, device):\n",
    "    model.eval()\n",
    "    data_iter = iter(val_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    idx = torch.randint(0, images.size(0), (1,)).item()\n",
    "    img = images[idx].unsqueeze(0).to(device)\n",
    "    label = labels[idx].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    img = img.squeeze().cpu().numpy()\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    if img.shape[0] == 3:\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Actual: {label.item()}, Predicted: {predicted.item()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8eced1-c7f0-4b9c-90df-b8b3607aff6a",
   "metadata": {},
   "source": [
    "| Label | Char | Label | Char | Label | Char | Label | Char | Label | Char | Label | Char | Label | Char | Label | Char |\n",
    "|-------|------|-------|------|-------|------|-------|------|-------|------|-------|------|-------|------|-------|------|\n",
    "| 0     | !    | 10    | Б    | 20    | Л    | 30    | Х    | 40    | д    | 50    | о    | 60    | э    | 70    | ҳ    |\n",
    "| 1     | ,    | 11    | В    | 21    | М    | 31    | Ч    | 41    | е    | 51    | П    | 61    | ю    | 71    | —    |\n",
    "| 2     | -    | 12    | Г    | 22    | Н    | 32    | Ш    | 42    | ж    | 52    | Р    | 62    | я    | 72    | ”    |\n",
    "| 3     | .    | 13    | Д    | 23    | О    | 33    | Э    | 43    | з    | 53    | С    | 63    | ё    | 73    | „    |\n",
    "| 4     | :    | 14    | Е    | 24    | п    | 34    | Ю    | 44    | и    | 54    | Т    | 64    | ў    |\n",
    "| 5     | ;    | 15    | Ж    | 25    | р    | 35    | Я    | 45    | й    | 55    | у    | 65    | Ғ    |\n",
    "| 6     | ?    | 16    | З    | 26    | с    | 36    | а    | 46    | к    | 56    | ф    | 66    | ғ    |\n",
    "| 7     | Ё    | 17    | И    | 27    | т    | 37    | б    | 47    | л    | 57    | х    | 67    | Қ    |\n",
    "| 8     | Ў    | 18    | Й    | 28    | У    | 38    | в    | 48    | м    | 58    | ч    | 68    | қ    |\n",
    "| 9     | А    | 19    | К    | 29    | Ф    | 39    | г    | 49    | н    | 59    | ш    | 69    | Ҳ    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84110372-fa37-4057-bd58-d2a873bba3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_prediction(model1, val_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948c1c1-849a-45d7-971a-c60cebfd78fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faa6d5-57e6-47fe-a6bf-c9f71b614238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585752f-d628-4c5d-a782-1d01e859e801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cc08f-95a5-48ce-9170-10de79622708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a19dc-0269-496c-be4e-3941a79e0808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c859695-c301-4f5e-b666-8f7dc9a516cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507d554-6953-4dce-984d-8a896968b02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf175-f576-4137-abd6-e698c55b4d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
